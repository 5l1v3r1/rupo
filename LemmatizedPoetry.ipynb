{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Anaconda2\\envs\\anastasyev-py3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "from collections import Counter\n",
    "import io\n",
    "from time import localtime, strftime\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Activation, Embedding, TimeDistributed, Dense, Dropout, Reshape, Merge, Highway, \\\n",
    "                        LSTM, Convolution2D, MaxPooling2D, BatchNormalization, SpatialDropout1D, Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import Callback\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "# from nltk import wordpunct_tokenize, sent_tokenize\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import gensim\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas_counter = pickle.load( open(\"lemmas_counter.pkl\", \"rb\") )\n",
    "dictionary = pickle.load( open(\"dictionary.pkl\", \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma2index = {}\n",
    "EMPTY_WORD = ''\n",
    "END_OF_LINE = '<eol>'\n",
    "lemma2index[EMPTY_WORD] = 0\n",
    "lemma2index[END_OF_LINE] = 1\n",
    "\n",
    "for lemma, _ in lemmas_counter.most_common(50000):\n",
    "    lemma2index[lemma] = len(lemma2index)\n",
    "\n",
    "index2lemma = { lemma2index[x] : x for x in lemma2index }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRAMMEMES = []\n",
    "\n",
    "def binarize(i, length):\n",
    "    vector = np.zeros(length + 1, dtype=np.int)\n",
    "    vector[i] = 1\n",
    "    return list(vector)\n",
    "    \n",
    "with open('ClassesNames.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split(' ')\n",
    "        GRAMMEMES.append({ x : binarize(i, len(fields)) for i, x in enumerate(fields) })\n",
    "        GRAMMEMES[-1][u'UNK'] = binarize(len(fields), len(fields))\n",
    "        \n",
    "GRAMMEMES_COUNT = sum(len(x) for x in GRAMMEMES)\n",
    "    \n",
    "def convert_tags(pos, tags):\n",
    "    tags = tags.split('|')\n",
    "    tags_vector = GRAMMEMES[0][pos] if pos in GRAMMEMES[0] else GRAMMEMES[0][u'UNK']\n",
    "    tags_vector = tags_vector[:]\n",
    "    for mapping in GRAMMEMES[1:]:\n",
    "        if any(tag in mapping for tag in tags):\n",
    "            for tag in tags:\n",
    "                if tag in mapping:\n",
    "                    tags_vector.extend(mapping[tag])\n",
    "        else:\n",
    "            tags_vector.extend(mapping[u'UNK'])\n",
    "    return tags_vector\n",
    "\n",
    "classesToValue = {}\n",
    "with open('classesList.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line[:-2]\n",
    "        pos, val = line[:line.index('\\t')], line[line.index('\\t') + 1:]\n",
    "        classesToValue[int(pos)] = val.replace('\\t', ' ')\n",
    "\n",
    "classesToValue[-1] = \"Unknown\"\n",
    "\n",
    "with open('classesMapping.txt', encoding='utf-8') as f:\n",
    "    classesMapping = { int(line[:-1].split('\\t')[1]) : int(line.split('\\t')[0]) for line in f }\n",
    "    \n",
    "classesMappingRev = {classesMapping[x] : x for x in classesMapping}\n",
    "\n",
    "space = re.compile('\\\\s+')\n",
    "def format_gram_value(gram_val):\n",
    "    first_tab = gram_val.find(' ')\n",
    "    if first_tab != -1:\n",
    "        pos, grval = gram_val[: first_tab], gram_val[first_tab + 1 :]\n",
    "        grval = grval.replace('-', ' ')\n",
    "        grval = space.subn(' ', grval)[0].strip()\n",
    "        grval = grval.replace(' ', '|')\n",
    "        if len(grval) != 0:\n",
    "            tags = { a.split('=')[0] : a.split('=')[1] if a != '_' else '_' for a in grval.split('|') }\n",
    "            if pos == 'VERB' and 'Tense' not in tags \\\n",
    "                    and (tags[\"VerbForm\"] == \"Fin\" and tags[\"Mood\"] == \"Ind\"\n",
    "                        or tags[\"VerbForm\"] == \"Conv\"):\n",
    "                tags[\"Tense\"] = \"Past\"\n",
    "            grval = ''\n",
    "            for tag in tags:\n",
    "                grval += tag + '=' + tags[tag] + '|'\n",
    "        else:\n",
    "            grval = '_ '\n",
    "        return pos + '\\t' + grval[:-1]\n",
    "    else:\n",
    "        return gram_val + '\\t_'\n",
    "\n",
    "index2tags_vector = {}\n",
    "for i in range(286 + 1):\n",
    "    if i >= 2:\n",
    "        pos, tags = format_gram_value(classesToValue[classesMapping[i] - 2]).split('\\t')\n",
    "        index2tags_vector[i] = convert_tags(pos, tags)\n",
    "    elif i == 0:\n",
    "        index2tags_vector[i] = list(np.zeros(GRAMMEMES_COUNT, dtype=np.int))\n",
    "    elif i == 1:\n",
    "        index2tags_vector[i] = convert_tags('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmatizedWord(object):\n",
    "    def __init__(self, lemma, gr_tag, word_form):\n",
    "        self.lemma = lemma\n",
    "        self.gr_tag = gr_tag\n",
    "        self.word_form = word_form\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"<Lemma = {}; GrTag = {}; WordForm = {}>\".format(self.lemma, self.gr_tag, self.word_form)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (self.lemma, self.gr_tag, self.word_form) == (other.lemma, other.gr_tag, other.word_form)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.lemma, self.gr_tag, self.word_form))\n",
    "        \n",
    "class LemmatizedVocabulary(object):\n",
    "    def __init__(self, voc_path, lemmas_counter=None, dictionary=None):\n",
    "        self.word_form2_lemmatization = {}\n",
    "        self.lemmatization2word_form = {}\n",
    "        self.lemmatizedWords = []\n",
    "        self.lemma2lemmatizedWord = {}\n",
    "        self.lemmatizedWordIndicies = {}\n",
    "        if voc_path:\n",
    "            with open(voc_path, encoding='utf8') as f:\n",
    "                for line in f:\n",
    "                    lemma, gr_tag, word_form = line.strip().split(' ')\n",
    "                    gr_tag = int(gr_tag)\n",
    "                    self.lemmatizedWords.append(LemmatizedWord(lemma, gr_tag, word_form))\n",
    "                    if word_form not in self.word_form2_lemmatization:\n",
    "                        self.word_form2_lemmatization[word_form] = []\n",
    "                    self.word_form2_lemmatization[word_form].append((lemma, gr_tag))\n",
    "                    if (lemma, gr_tag) not in self.lemmatization2word_form:\n",
    "                        self.lemmatization2word_form[(lemma, gr_tag)] = []\n",
    "                    self.lemmatization2word_form[(lemma, gr_tag)].append(word_form)\n",
    "                    if lemma not in self.lemma2lemmatizedWord:\n",
    "                        self.lemma2lemmatizedWord[lemma] = []\n",
    "                    self.lemma2lemmatizedWord[lemma].append(LemmatizedWord(lemma, gr_tag, word_form))\n",
    "        else:\n",
    "            for pair in dictionary:\n",
    "                if pair[0] not in self.lemma2lemmatizedWord:\n",
    "                    self.lemma2lemmatizedWord[pair[0]] = []\n",
    "                self.lemma2lemmatizedWord[pair[0]].append(LemmatizedWord(pair[0], pair[1], dictionary[pair]))\n",
    "            self.word_form2_lemmatization = {dictionary[pair] : LemmatizedWord(pair[0], pair[1], dictionary[pair]) \n",
    "                                             for pair in dictionary}\n",
    "            self.lemmatization2word_form = {self.word_form2_lemmatization[x] : x for x in self.word_form2_lemmatization}\n",
    "            for lemma, _ in lemmas_counter.most_common():\n",
    "                self.lemmatizedWords.extend(self.lemma2lemmatizedWord[lemma])\n",
    "        self.lemmatizedWordIndicies = {x : i for i, x in enumerate(self.lemmatizedWords)}\n",
    "        self.index2lemmatizedWord = {i : x for i, x in enumerate(self.lemmatizedWords)}\n",
    "                \n",
    "    def get_word_form(self, lemma, gr_tag):\n",
    "        return self.lemmatization2word_form[(lemma, gr_tag)] \\\n",
    "                if (lemma, gr_tag) in self.lemmatization2word_form[(lemma, gr_tag)] \\\n",
    "                else None\n",
    "    \n",
    "    def get_lemmatization(self, word_form):\n",
    "        return self.word_form2_lemmatization[word_form] \\\n",
    "                if word_form in self.word_form2_lemmatization[word_form] \\\n",
    "                else None\n",
    "    \n",
    "    def choice_word(self):\n",
    "        return self.lemmatizedWords[np.random.randint(0, len(self.lemmatizedWords))]\n",
    "    \n",
    "    def get_paradigm(self, lemma):\n",
    "        return self.lemma2lemmatizedWord[lemma] if lemma in self.lemma2lemmatizedWord else None\n",
    "    \n",
    "    def get_word_form_index(self, lemmatizedWord):\n",
    "        return self.lemmatizedWordIndicies[lemmatizedWord] \\\n",
    "            if lemmatizedWord in self.lemmatizedWordIndicies \\\n",
    "            else len(self.lemmatizedWordIndicies)\n",
    "            \n",
    "    def get_word_form_by_index(self, index):\n",
    "        return self.index2lemmatizedWord[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizedVocabulary = LemmatizedVocabulary(None, lemmas_counter, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOFTMAX_SIZE = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def hard_tanh(x):\n",
    "    one = K.constant(1)\n",
    "    neg_one = K.constant(-1)\n",
    "    return K.minimum(K.maximum(x, neg_one), one)\n",
    "\n",
    "get_custom_objects().update({'custom_activation': Activation(hard_tanh)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\anastasyev-py3\\lib\\site-packages\\keras\\activations.py:89: UserWarning: Do not pass a layer instance (such as Activation) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  ).format(identifier=identifier.__class__.__name__))\n",
      "C:\\Anaconda2\\envs\\anastasyev-py3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "# Grammemes + embeddings model\n",
    "words = Input(shape=(None,), name='words')\n",
    "words_embedding = SpatialDropout1D(0.3)(Embedding(len(lemma2index) + 1, 150, name='embeddings')(words))\n",
    "  \n",
    "grammemes_input = Input(shape=(None, GRAMMEMES_COUNT), name='grammemes')\n",
    "grammemes_layer = Masking(mask_value=0.)(grammemes_input)\n",
    "\n",
    "# Добавить ещё слой, сделать функцию активации\n",
    "grammemes_layer = Dense(25, activation=Activation(hard_tanh))(grammemes_layer)\n",
    "grammemes_layer = Dense(25, activation=Activation(hard_tanh))(grammemes_layer)\n",
    "    \n",
    "layer = Merge(mode='concat', name='LSTM_input')([words_embedding, grammemes_layer])\n",
    "\n",
    "layer = LSTM(368, dropout=.2, recurrent_dropout=.2, return_sequences=True, name='LSTM_1')(layer)\n",
    "layer = LSTM(368, dropout=.2, recurrent_dropout=.2, return_sequences=False, name='LSTM_2')(layer)\n",
    "\n",
    "output = Dense(SOFTMAX_SIZE + 1, activation='softmax')(layer)\n",
    "\n",
    "model = Model(inputs=[words, grammemes_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(name + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENT_LEN = 10\n",
    "\n",
    "def get_word_index(word):\n",
    "    return min(lemmatizedVocabulary.get_word_form_index(word), SOFTMAX_SIZE)\n",
    "\n",
    "class BatchGenerator():\n",
    "    def __init__(self, fname, batch_size):\n",
    "        self.fname = fname\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def __generate_seqs(sents):\n",
    "        seqs, next_words = [], []\n",
    "        for sent in sents:\n",
    "            sent = sent[::-1]\n",
    "            for i in range(1, len(sent)):\n",
    "                if lemmatizedVocabulary.get_word_form_index(sent[i]) >= SOFTMAX_SIZE:\n",
    "                    continue\n",
    "                seqs.append(sent[max(0, i - SENT_LEN) : i])\n",
    "                next_words.append(sent[i])\n",
    "        return seqs, next_words\n",
    "        \n",
    "    @staticmethod\n",
    "    def __to_tensor(sents):\n",
    "        sents, next_words = BatchGenerator.__generate_seqs(sents)\n",
    "        max_len = max(len(sent) for sent in sents)\n",
    "        X_emb = np.zeros((len(sents), max_len), dtype=np.int)\n",
    "        X_grammemes = np.zeros((len(sents), max_len, GRAMMEMES_COUNT), dtype=np.int)\n",
    "        y = np.zeros(len(sents), dtype=np.int)\n",
    "        for i in range(len(sents)):\n",
    "            X_emb[i, -len(sents[i]):] = [get_word_index(x) for x in sents[i]]\n",
    "            X_grammemes[i, -len(sents[i]):] = [index2tags_vector[x.gr_tag] for x in sents[i]]\n",
    "            y[i] = get_word_index(next_words[i])\n",
    "        return X_emb, X_grammemes, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_tensor(words_indices):\n",
    "        return BatchGenerator.__to_tensor([idx2word[ind] for ind in words_indices])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        sents = [[]]\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    sents.append([])\n",
    "                else:\n",
    "                    word, lemma, pos, tags, index = line.split('\\t')\n",
    "                    sents[-1].append(LemmatizedWord(lemma + '_' + pos, int(index), word))\n",
    "                if len(sents) >= self.batch_size:\n",
    "                    yield BatchGenerator.__to_tensor(sents)\n",
    "                    sents = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvalCallback(Callback):\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "        \n",
    "    def write(self, message):\n",
    "        print('[{}] {}'.format(strftime(\"%H:%M:%S\", localtime()), message))\n",
    "        \n",
    "    def _sample(self, prob, temperature=1.0):\n",
    "        prob = prob[:-1] # Не хотим предсказывать UNKNOWN_WORD\n",
    "        prob = np.log(prob) / temperature\n",
    "        prob = np.exp(prob) / np.sum(np.exp(prob))\n",
    "        return np.random.choice(len(prob), p=prob)\n",
    "\n",
    "    def _generate(self):\n",
    "        cur_sent = [lemmatizedVocabulary.get_word_form_by_index(np.random.randint(0, SOFTMAX_SIZE))]\n",
    "        for i in range(10):\n",
    "            X_emb = np.zeros((1, len(cur_sent)))\n",
    "            X_gr = np.zeros((1, len(cur_sent), GRAMMEMES_COUNT))\n",
    "            for ind, word in enumerate(cur_sent):\n",
    "                X_emb[0, ind] = get_word_index(word)\n",
    "                X_gr[0, ind] = index2tags_vector[word.gr_tag]\n",
    "\n",
    "            preds = model.predict([X_emb, X_gr], verbose=0)[0]\n",
    "            cur_sent.append(lemmatizedVocabulary.get_word_form_by_index(self._sample(preds)))\n",
    "\n",
    "        print('Sentence', end=': ')\n",
    "        for word in cur_sent[::-1]:\n",
    "            print(word.word_form, end=' ')\n",
    "        print()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self._generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_generator = BatchGenerator('Data/Poetry_preds.txt_lemmatized_train', 10000)\n",
    "name = 'Lemmatized_hard_tanh'\n",
    "with open(name + '_log.txt', 'a') as f:\n",
    "    with redirect_stdout(f):\n",
    "        callback = EvalCallback(name)\n",
    "        for big_epoch in range(1000):\n",
    "            print('------------Big Epoch {}------------'.format(big_epoch))\n",
    "            for epoch, (X1, X2, y) in enumerate(batch_generator):\n",
    "                model.fit([X1, X2], y, batch_size=768, epochs=1, verbose=2, callbacks=[callback])\n",
    "                if epoch != 0 and epoch % 10 == 0:\n",
    "                    model.save_weights(name + '_model.h5')\n",
    "                f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading voc\n",
      "loading lstm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\anastasyev-py3\\lib\\site-packages\\keras\\engine\\topology.py:1231: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading generator\n"
     ]
    }
   ],
   "source": [
    "from rupo.main.vocabulary import Vocabulary\n",
    "from rupo.generate.lemmatized_vocabulary import LemmatizedVocabulary, LemmatizedWord\n",
    "from rupo.generate import lstm\n",
    "from rupo.generate import generator\n",
    "\n",
    "print('loading voc')\n",
    "vocab_dump_file = \"rupo_files/voc.pickle\"\n",
    "vocabulary = Vocabulary(vocab_dump_file)\n",
    "\n",
    "print('loading lstm')\n",
    "lstm_container = lstm.LSTM_Container('rupo_files/Lemmatized')\n",
    "\n",
    "print('loading generator')\n",
    "gen = generator.Generator(lstm_container, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так толку мне теперь грустить\n",
      "Что будет это прожито\n",
      "Не суждено кружить в пути\n",
      "Почувствовав боль бомжика\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gen.generate_poem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\anastasyev-py3\\lib\\site-packages\\rupo\\generate\\filters.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  model /= np.sum(model)\n",
      "C:\\Anaconda2\\envs\\anastasyev-py3\\lib\\site-packages\\rupo\\generate\\generator.py:129: RuntimeWarning: invalid value encountered in less\n",
      "  return choice(range(len(model)), 1, p=model)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Плечом к плечу рукой махнул\n",
      "Но вот она опять давно\n",
      "И на удачу распахну\n",
      "Окно и снова все равно\n",
      "Придет прощайте мама и\n",
      "Мне друг за что вам вы скажи\n",
      "Твои давно по кораблю\n",
      "Шуршат а я дышу люблю\n",
      "Тебя пытаться объяснить\n",
      "Все не твое и жду я и\n",
      "Мне все решать мне одному\n",
      "Не крошка ты меня простить\n",
      "За совесть я хочу в простор\n",
      "Больно и чтоб твоих обжор\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gen.generate_poem(rhyme_pattern='ababccddeffegg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На все кричать и убивать\n",
      "Она в вине тебя затмит\n",
      "Ты спишь и хочешь танцевать\n",
      "Что хочешь для меня найти\n",
      "Мне хуже будешь ты никак\n",
      "Кто рядом так ведь он пока\n",
      "Что то подруги выпили\n",
      "И волны снова выплюнут\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gen.generate_poem(rhyme_pattern='ababccdd'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
